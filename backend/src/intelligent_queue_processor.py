"""
ğŸ§  INTELLIGENT QUEUE PROCESSOR
Processes SQS messages with true capacity awareness and job order preservation
"""

print("ğŸ§  Intelligent Queue Processor: Starting imports...")

import json
print("âœ… json imported")

import boto3
print("âœ… boto3 imported")

import os
print("âœ… os imported")

import uuid
print("âœ… uuid imported")

import base64
print("âœ… base64 imported")

from datetime import datetime
print("âœ… datetime imported")

import logging
print("âœ… logging imported")

from capacity_manager import get_capacity_manager
print("âœ… capacity_manager imported")

print("ğŸ§  Intelligent Queue Processor: All imports successful, configuring logging...")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("ğŸ§  Intelligent Queue Processor: Logging configured, initializing AWS clients...")

# AWS clients
bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')
print("âœ… bedrock_client initialized")

s3_client = boto3.client('s3')
print("âœ… s3_client initialized")

dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
print("âœ… dynamodb resource initialized")

print("ğŸ§  Intelligent Queue Processor: Loading environment variables...")

# Environment variables
S3_BUCKET_NAME = os.environ.get('S3_BUCKET_NAME')
NOVA_CANVAS_MODEL = os.environ.get('NOVA_CANVAS_MODEL', 'amazon.nova-canvas-v1:0')
JOB_TRACKING_TABLE = os.environ.get('JOB_TRACKING_TABLE')
TEMPLATE_EVENT_NAME = os.environ.get('TEMPLATE_EVENT_NAME', 'AWS Event')
TEMPLATE_LOGOS_JSON = os.environ.get('TEMPLATE_LOGOS_JSON', '[]')

print(f"âœ… Environment variables loaded:")
print(f"   S3_BUCKET_NAME: {S3_BUCKET_NAME}")
print(f"   NOVA_CANVAS_MODEL: {NOVA_CANVAS_MODEL}")
print(f"   JOB_TRACKING_TABLE: {JOB_TRACKING_TABLE}")

print("ğŸ§  Intelligent Queue Processor: Initializing DynamoDB table...")

# DynamoDB table (conditional initialization for testing)
job_table = None
if JOB_TRACKING_TABLE:
    job_table = dynamodb.Table(JOB_TRACKING_TABLE)
    print(f"âœ… DynamoDB table initialized: {JOB_TRACKING_TABLE}")
else:
    print("âš ï¸ JOB_TRACKING_TABLE not set - DynamoDB operations will be disabled")

print("ğŸ‰ Intelligent Queue Processor: Initialization complete!")

def lambda_handler(event, context):
    """
    ğŸ§  INTELLIGENT QUEUE PROCESSOR
    
    This is the core of the intelligent system:
    1. Check capacity BEFORE calling Bedrock
    2. Only process jobs when we KNOW we have capacity
    3. Preserve job order - first in, first out
    4. Learn from successes and failures to optimize capacity
    """
    try:
        print(f"ğŸ§  INTELLIGENT PROCESSOR STARTED - Request ID: {context.aws_request_id}")
        logger.info(f"ğŸ§  Intelligent Queue Processor started - Request ID: {context.aws_request_id}")
        
        print(f"ğŸ“¥ RAW EVENT: {json.dumps(event, default=str)}")
        logger.info(f"ğŸ“¥ Received event: {json.dumps(event, default=str)}")
        
        # Check if we have SQS records
        if 'Records' not in event:
            print("âŒ NO RECORDS FOUND IN EVENT")
            logger.error("âŒ No 'Records' found in event - this should be an SQS event")
            return {'statusCode': 400, 'body': 'Invalid event structure'}
        
        records = event['Records']
        print(f"ğŸ“¨ RECEIVED {len(records)} MESSAGES")
        logger.info(f"ğŸ“¨ Intelligent Processor: Received {len(records)} messages")
        
        # ğŸ§  CRITICAL: Process only the FIRST message to preserve order
        if len(records) > 1:
            print(f"ğŸ¯ ORDER PRESERVATION: Got {len(records)} messages, processing ONLY the first to maintain order")
            logger.info(f"ğŸ¯ Order preservation: Processing only first of {len(records)} messages")
        
        record = records[0]  # Always process first message only
        
        try:
            print(f"ğŸ“ PROCESSING FIRST MESSAGE")
            logger.info(f"ğŸ“ Processing first message")
            
            print(f"ğŸ“ RECORD: {json.dumps(record, default=str)}")
            logger.info(f"ğŸ“ Record structure: {json.dumps(record, default=str)}")
            
            # Parse SQS message
            print(f"ğŸ“ PARSING MESSAGE BODY...")
            message_body = json.loads(record['body'])
            print(f"ğŸ“ MESSAGE BODY: {json.dumps(message_body, default=str)}")
            logger.info(f"ğŸ“ Message body: {json.dumps(message_body, default=str)}")
            
            job_id = message_body['job_id']
            prompt = message_body['prompt']
            
            # Enhanced user correlation fields
            user_number = message_body.get('user_number', 1)
            display_name = message_body.get('display_name', f'Test User #{user_number}')
            device_id = message_body.get('device_id', 'unknown')
            session_id = message_body.get('session_id', f'{device_id}_user_{user_number:03d}_override1')
            
            print(f"ğŸ¯ PROCESSING JOB {job_id} for {display_name}: {prompt[:50]}...")
            logger.info(f"ğŸ¯ Processing job {job_id} for {display_name}: {prompt[:50]}...")
            
            # ğŸ§  INTELLIGENT CAPACITY CHECK - This is the core intelligence!
            print(f"ğŸ§  LOADING CAPACITY MANAGER...")
            capacity_manager = get_capacity_manager()
            
            print(f"ğŸ§  CHECKING BEDROCK CAPACITY...")
            if not capacity_manager.can_process_job():
                print(f"â³ NO CAPACITY AVAILABLE - Job {job_id} will wait in queue")
                print(f"ğŸ¯ INTELLIGENT DECISION: Not calling Bedrock, preserving job order")
                logger.info(f"â³ No capacity for job {job_id} - leaving in queue to preserve order")
                
                # ğŸ¯ CRITICAL: Don't delete the message - it stays in queue
                # This preserves job order and prevents Bedrock spam
                return {
                    'statusCode': 200, 
                    'body': f'Job {job_id} waiting for capacity - preserved in queue'
                }
            
            # ğŸš€ WE HAVE CAPACITY - Process the job!
            print(f"âœ… CAPACITY AVAILABLE - Processing job {job_id}")
            logger.info(f"âœ… Capacity available - processing job {job_id}")
            
            # Mark job as started in capacity manager
            capacity_manager.job_started(job_id)
            
            # Update job status to processing
            print(f"ğŸ“Š UPDATING JOB STATUS TO PROCESSING...")
            update_job_status(job_id, 'processing', {
                'user_number': user_number,
                'display_name': display_name,
                'device_id': device_id,
                'session_id': session_id,
                'started_at': datetime.now().isoformat()
            })
            print(f"âœ… JOB STATUS UPDATED TO PROCESSING")
            
            # Generate card with Bedrock
            print(f"ğŸ¨ CALLING BEDROCK WITH CONFIDENCE (we know we have capacity)...")
            result = generate_card_with_bedrock(prompt, job_id, session_id, user_number, display_name, device_id, capacity_manager)
            print(f"ğŸ¨ BEDROCK CALL RESULT: {result}")
            
            if result['success']:
                print(f"ğŸ‰ JOB {job_id} COMPLETED SUCCESSFULLY")
                logger.info(f"ğŸ‰ Job {job_id} completed successfully for {display_name}")
                
                # Update job status to completed
                update_job_status(job_id, 'completed', {
                    'user_number': user_number,
                    'display_name': display_name,
                    'device_id': device_id,
                    'session_id': session_id,
                    's3_url': result['s3_url'],
                    's3_key': result['s3_key'],
                    'completed_at': datetime.now().isoformat()
                })
                
                # ğŸ§  LEARNING: Job succeeded, update capacity manager
                capacity_manager.job_completed_success(job_id)
                
            else:
                print(f"âŒ JOB {job_id} FAILED: {result['error']}")
                logger.error(f"âŒ Job {job_id} failed for {display_name}: {result['error']}")
                
                # Update job status to failed
                update_job_status(job_id, 'failed', {
                    'user_number': user_number,
                    'display_name': display_name,
                    'device_id': device_id,
                    'session_id': session_id,
                    'error': result['error'],
                    'failed_at': datetime.now().isoformat()
                })
                
                # ğŸ§  LEARNING: Job failed, update capacity manager
                capacity_manager.job_completed_error(job_id, result['error'])
                    
        except Exception as e:
            print(f"âŒ ERROR PROCESSING MESSAGE: {str(e)}")
            logger.error(f"âŒ Error processing message: {str(e)}")
            logger.error(f"âŒ Record content: {json.dumps(record, default=str)}")
            
            # Try to update job status if we can extract job_id
            try:
                message_body = json.loads(record['body'])
                job_id = message_body.get('job_id')
                if job_id:
                    print(f"ğŸ“Š UPDATING FAILED JOB {job_id}")
                    update_job_status(job_id, 'failed', {
                        'error': f'Processing error: {str(e)}',
                        'failed_at': datetime.now().isoformat()
                    })
                    
                    # Update capacity manager
                    capacity_manager = get_capacity_manager()
                    capacity_manager.job_completed_error(job_id, str(e))
                    
            except Exception as inner_e:
                print(f"âŒ COULD NOT UPDATE JOB STATUS: {str(inner_e)}")
                logger.error(f"âŒ Could not update job status for failed record: {str(inner_e)}")
            
            # Re-raise to let SQS handle retry
            raise e
        
        print(f"âœ… INTELLIGENT PROCESSOR COMPLETED SUCCESSFULLY")
        logger.info(f"âœ… Intelligent Queue Processor completed successfully")
        return {'statusCode': 200, 'body': 'Message processed successfully'}
        
    except Exception as e:
        print(f"âŒ FATAL ERROR IN INTELLIGENT PROCESSOR: {str(e)}")
        logger.error(f"âŒ Fatal error in intelligent processor: {str(e)}")
        logger.error(f"âŒ Event: {json.dumps(event, default=str)}")
        
        # Let SQS handle retry - this preserves message order
        raise e

def generate_card_with_bedrock(prompt, job_id, session_id, user_number, display_name, device_id, capacity_manager):
    """
    Generate trading card using Bedrock Nova Canvas with intelligent capacity management
    """
    try:
        print(f"ğŸ¨ STARTING NOVA CANVAS GENERATION FOR JOB {job_id} - {display_name}")
        logger.info(f"ğŸ¨ Starting Nova Canvas generation for job {job_id} - {display_name}")
        
        # Prepare the request payload for Nova Canvas
        request_payload = {
            "taskType": "TEXT_IMAGE",
            "textToImageParams": {
                "text": prompt
            },
            "imageGenerationConfig": {
                "numberOfImages": 1,
                "quality": "premium",
                "height": 720,
                "width": 1280,
                "cfgScale": 7.0,
                "seed": 42
            }
        }
        
        print(f"ğŸ¨ CALLING BEDROCK NOVA CANVAS FOR JOB {job_id}")
        print(f"ğŸ¨ MODEL: {NOVA_CANVAS_MODEL}")
        print(f"ğŸ¨ PAYLOAD: {json.dumps(request_payload)}")
        logger.info(f"ğŸ¨ Calling Bedrock Nova Canvas for job {job_id}")
        
        # ğŸš€ CONFIDENT BEDROCK CALL: We know we have capacity!
        response = bedrock_client.invoke_model(
            modelId=NOVA_CANVAS_MODEL,
            body=json.dumps(request_payload),
            contentType='application/json'
        )
        
        print(f"âœ… BEDROCK RESPONSE RECEIVED FOR JOB {job_id}")
        
        # Parse response
        response_body = json.loads(response['body'].read())
        print(f"âœ… RESPONSE PARSED FOR JOB {job_id}")
        logger.info(f"âœ… Nova Canvas response received for job {job_id}")
        
        if 'images' in response_body and len(response_body['images']) > 0:
            print(f"âœ… IMAGE DATA FOUND FOR JOB {job_id}")
            # Get the base64 image data
            image_data = base64.b64decode(response_body['images'][0])
            
            # Generate enhanced S3 key with user correlation
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            s3_key = f"cards/{session_id}_card_1_{timestamp}.png"
            
            print(f"ğŸ’¾ UPLOADING TO S3: {s3_key}")
            logger.info(f"ğŸ’¾ Uploading to S3: {s3_key}")
            
            # Upload to S3
            s3_client.put_object(
                Bucket=S3_BUCKET_NAME,
                Key=s3_key,
                Body=image_data,
                ContentType='image/png',
                Metadata={
                    'job_id': job_id,
                    'user_number': str(user_number),
                    'display_name': display_name,
                    'device_id': device_id,
                    'session_id': session_id,
                    'generated_at': timestamp
                }
            )
            
            # Generate S3 URL
            s3_url = f"https://{S3_BUCKET_NAME}.s3.us-east-1.amazonaws.com/{s3_key}"
            
            print(f"ğŸ‰ CARD GENERATED SUCCESSFULLY FOR JOB {job_id} - {display_name}")
            print(f"ğŸ“ S3 URL: {s3_url}")
            logger.info(f"ğŸ‰ Card generated successfully for job {job_id} - {display_name}")
            logger.info(f"ğŸ“ S3 URL: {s3_url}")
            
            return {
                'success': True,
                's3_url': s3_url,
                's3_key': s3_key,
                'job_id': job_id,
                'user_number': user_number,
                'display_name': display_name,
                'device_id': device_id,
                'session_id': session_id
            }
        else:
            error_msg = "No images returned from Nova Canvas"
            print(f"âŒ {error_msg} FOR JOB {job_id}")
            logger.error(f"âŒ {error_msg} for job {job_id}")
            return {'success': False, 'error': error_msg}
            
    except Exception as e:
        error_str = str(e)
        
        if 'ThrottlingException' in error_str or 'TooManyRequestsException' in error_str:
            print(f"ğŸ¤” UNEXPECTED THROTTLING FOR JOB {job_id} - Our capacity estimate was wrong!")
            print(f"ğŸ§  LEARNING MOMENT: Reducing capacity estimate")
            logger.warning(f"ğŸ¤” Unexpected throttling for job {job_id} - learning from this")
            
            # ğŸ§  LEARNING: We were wrong about capacity
            capacity_manager.job_completed_throttled(job_id)
            
            # Let SQS handle retry - job will be processed when we have real capacity
            raise e
            
        elif 'ServiceQuotaExceededException' in error_str:
            print(f"ğŸš« SERVICE QUOTA EXCEEDED FOR JOB {job_id}")
            logger.error(f"ğŸš« Service quota exceeded for job {job_id}")
            
            # This is also a capacity issue
            capacity_manager.job_completed_throttled(job_id)
            raise e
            
        else:
            # Other errors (validation, etc.) - don't retry
            error_msg = f"Bedrock generation failed: {error_str}"
            print(f"âŒ OTHER ERROR FOR JOB {job_id}: {error_str}")
            logger.error(f"âŒ {error_msg} for job {job_id}")
            return {'success': False, 'error': error_msg}

def update_job_status(job_id, status, metadata=None):
    """
    Update job status in DynamoDB with enhanced user correlation metadata
    """
    if not job_table:
        print(f"âš ï¸ CANNOT UPDATE JOB {job_id} - DYNAMODB TABLE NOT AVAILABLE")
        logger.warning(f"âš ï¸ Cannot update job {job_id} - DynamoDB table not available")
        return
    
    try:
        print(f"ğŸ“Š UPDATING JOB {job_id} STATUS TO: {status}")
        
        # Get existing job record to preserve created_at timestamp
        response = job_table.get_item(Key={'jobId': job_id})
        if 'Item' in response:
            created_at = response['Item'].get('created_at')
            print(f"ğŸ“Š FOUND EXISTING JOB {job_id}, CREATED_AT: {created_at}")
        else:
            created_at = datetime.now().isoformat()
            print(f"ğŸ“Š NEW JOB {job_id}, SETTING CREATED_AT: {created_at}")
        
        # Prepare update data with enhanced metadata
        update_data = {
            'jobId': job_id,
            'status': status,
            'updated_at': datetime.now().isoformat(),
            'created_at': created_at
        }
        
        # Add metadata if provided
        if metadata:
            update_data.update(metadata)
            print(f"ğŸ“Š ADDED METADATA: {json.dumps(metadata, default=str)}")
        
        # Handle reserved keywords for DynamoDB
        reserved_keywords = {
            'status': '#job_status'
        }
        
        # Build update expression
        update_expression = "SET "
        expression_attribute_names = {}
        expression_attribute_values = {}
        
        for key, value in update_data.items():
            if key == 'jobId':  # Skip the key
                continue
            elif key == 'status':
                # Handle reserved keyword
                update_expression += "#job_status = :job_status, "
                expression_attribute_names["#job_status"] = "status"
                expression_attribute_values[":job_status"] = value
            elif key in reserved_keywords:
                # Handle other reserved keywords
                attr_name = reserved_keywords[key]
                update_expression += f"{attr_name} = :{key}, "
                expression_attribute_names[attr_name] = key
                expression_attribute_values[f":{key}"] = value
            else:
                # Regular attributes
                update_expression += f"{key} = :{key}, "
                expression_attribute_values[f":{key}"] = value
        
        update_expression = update_expression.rstrip(', ')
        
        print(f"ğŸ“Š UPDATE EXPRESSION: {update_expression}")
        print(f"ğŸ“Š ATTRIBUTE VALUES: {json.dumps(expression_attribute_values, default=str)}")
        
        # Only include ExpressionAttributeNames if we have reserved keywords
        update_params = {
            'Key': {'jobId': job_id},
            'UpdateExpression': update_expression,
            'ExpressionAttributeValues': expression_attribute_values
        }
        
        if expression_attribute_names:
            update_params['ExpressionAttributeNames'] = expression_attribute_names
            print(f"ğŸ“Š ATTRIBUTE NAMES: {json.dumps(expression_attribute_names)}")
        
        job_table.update_item(**update_params)
        
        print(f"âœ… JOB {job_id} STATUS UPDATED TO: {status}")
        logger.info(f"ğŸ“Š Job {job_id} status updated to: {status}")
        
    except Exception as e:
        print(f"âŒ FAILED TO UPDATE JOB {job_id} STATUS: {str(e)}")
        logger.error(f"âŒ Failed to update job {job_id} status: {str(e)}")
